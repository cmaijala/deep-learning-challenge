# deep-learning-challenge


Alphabet Soup Charity Analysis
Overview of the Analysis
This project aims to develop a deep learning model to predict the success of charity applications based on historical data. By identifying the factors that contribute to successful applications, the model can help organizations optimize their fundraising efforts and make informed decisions.

Results
Data Preprocessing
Target Variable:
The target variable for the model is IS_SUCCESSFUL, indicating whether an application was successful (1) or not (0).

Feature Variables:
The features used for training the model include:

APPLICATION_TYPE: Type of application.
AFFILIATION: Organizationâ€™s affiliation type.
CLASSIFICATION: Charity classification.
USE_CASE: Use case for the donation.
ORGANIZATION: Organization type.
INCOME_AMT: Income amount category.
SPECIAL_CONSIDERATIONS: Whether there are special considerations for the application.
ASK_AMT: Log-transformed version of the amount requested for the donation.
Removed Variables:
The following columns were removed from the dataset:

EIN: Unique identifier for organizations, not useful for prediction.
NAME: Organization name, not predictive of application success.
STATUS: Redundant column with only two possible values, offering no additional insights.
Compiling, Training, and Evaluating the Model
Model Architecture:
The model uses the following architecture:

Input Layer: 128 neurons with ReLU activation.
Hidden Layer: 64 neurons with ReLU activation.
Output Layer: 1 neuron with sigmoid activation for binary classification.
Activation Functions:

ReLU was used for the input and hidden layers to handle non-linear relationships effectively and avoid vanishing gradients.
Sigmoid was used in the output layer to output probabilities between 0 and 1 for classification.
Model Performance:
Despite multiple optimization attempts, the model was unable to achieve the target accuracy of 75%. The final accuracy on the test set was approximately 72%, indicating that while the model learned some patterns, it was not able to generalize well enough to reach the desired performance.

Optimization Steps:

Feature Engineering:
Rare categories in APPLICATION_TYPE and CLASSIFICATION were binned into an "OTHER" category to reduce noise.
ASK_AMT was log-transformed to reduce skewness and improve model performance.
Model Tuning:
Adjusted the number of neurons and layers to balance complexity and performance.
Trained the model for 50 epochs with a batch size of 32, ensuring convergence without overfitting.
Experimented with different numbers of neurons, layers, and activation functions.
Summary
Results:
The deep learning model achieved a final accuracy of approximately 72%, which is below the target threshold of 75%. This suggests that while the model was moderately effective at predicting the success of charity applications, there may be limitations in the feature set or the model architecture.

Alternative Model Recommendations:
To address the performance gap, alternative models could be explored:

Random Forest Classifier:
Random Forests handle categorical features and outliers effectively and provide feature importance insights, making them highly interpretable.
Gradient Boosting Machines (e.g., XGBoost or LightGBM):
These models excel at handling high-dimensional data and often outperform other models in structured data tasks. They are robust to overfitting and handle imbalanced datasets well.
Both models could provide stronger performance and better interpretability compared to the deep learning approach, especially with a relatively small dataset.

